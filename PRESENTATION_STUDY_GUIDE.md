# üìö MCP Network Optimizer - Complete Presentation Study Guide

> **Purpose**: This comprehensive guide covers every technical concept, parameter, and configuration in the MCP Network Optimizer project. Use this to prepare for technical questions during your presentation.

---

## Table of Contents

1. [Project Overview](#1-project-overview)
2. [Core Technologies & Concepts](#2-core-technologies--concepts)
3. [TCP/IP Stack Parameters](#3-tcpip-stack-parameters)
4. [Traffic Control (TC) Concepts](#4-traffic-control-tc-concepts)
5. [Firewall & Security (iptables/nftables)](#5-firewall--security-iptablesnftables)
6. [Network Performance Concepts](#6-network-performance-concepts)
7. [Optimization Profiles](#7-optimization-profiles)
8. [System Architecture](#8-system-architecture)
9. [Common Technical Questions & Answers](#9-common-technical-questions--answers)

---

## 1. Project Overview

### What is MCP Network Optimizer?

**MCP Network Optimizer** is an AI-driven system that allows LLMs (Large Language Models) to safely optimize Linux network configurations through the Model Context Protocol (MCP).

**Key Features:**
- 29 Configuration Cards covering all aspects of Linux networking
- 5 Research-backed optimization profiles (Gaming, Streaming, Video Calls, Bulk Transfer, Server)
- Automatic validation, rendering, and safe application with rollback
- Policy-driven safety constraints

**What Problems Does It Solve?**
- **Complexity**: Network optimization requires deep Linux knowledge
- **Safety**: Manual configuration risks breaking network connectivity
- **Accessibility**: Makes network optimization available to AI agents

---

## 2. Core Technologies & Concepts

### 2.1 Model Context Protocol (MCP)

**What it is:**
- A protocol that allows AI models to interact with external tools and data sources
- Similar to how a web browser uses HTTP to fetch web pages
- Enables Claude/GPT-4 to call functions in your system

**Why we use it:**
- Allows LLMs to discover network state
- Enables LLMs to plan and apply optimizations
- Provides structured communication between AI and system tools

### 2.2 FastMCP

**What it is:**
- A Python framework for building MCP servers quickly
- Handles JSON-RPC communication, tool registration, and resource management

**Our Usage:**
- `server/main.py`: Runs the MCP server
- `server/registry.py`: Registers all tools (functions) and resources (data) the LLM can access

### 2.3 Pydantic

**What it is:**
- Python library for data validation using type hints
- Ensures data integrity at runtime

**Our Usage:**
- `server/schema/models.py`: Defines all data structures
  - `ParameterPlan`: User's optimization request
  - `RenderedPlan`: Generated system commands
  - `ChangeReport`: Execution results
  - `ValidationResult`: Validation outcomes

---

## 3. TCP/IP Stack Parameters

### 3.1 TCP Congestion Control Algorithms

**What it is:**
Congestion control determines how TCP responds when the network becomes congested (too much traffic).

#### BBR (Bottleneck Bandwidth and Round-trip propagation time)
- **Developed by:** Google (Neal Cardwell et al., 2016)
- **How it works:** Measures available bandwidth and RTT, sends at optimal rate
- **Best for:** High-throughput applications, streaming, gaming
- **Advantages:** 2-25x better throughput on bufferbloated networks
- **Our usage:** Default in all profiles

#### CUBIC
- **What it is:** Linux default congestion control algorithm (before BBR)
- **How it works:** Uses cubic function to probe for bandwidth
- **Best for:** General-purpose, stable networks
- **When to use:** Fallback when BBR isn't available

#### Reno
- **What it is:** Classic TCP congestion control
- **How it works:** Additive increase, multiplicative decrease (AIMD)
- **Best for:** Legacy compatibility, simple networks
- **Limitations:** Poor performance on high-bandwidth, high-latency networks

#### Others
- **DCTCP**: Data Center TCP - for data centers with ECN support
- **Hybla**: Optimized for satellite/high-latency links
- **Vegas**: Focuses on RTT changes rather than packet loss
- **Westwood**: Optimized for wireless networks

**Configuration:**
```bash
sysctl -w net.ipv4.tcp_congestion_control=bbr
```

### 3.2 TCP Buffer Sizes

#### tcp_rmem (Receive Memory)
**What it is:** Controls how much data can be buffered for incoming TCP connections

**Format:** `"min default max"` (3 values in bytes)
- **min**: Minimum buffer allocated to each socket (typically 4096 bytes = 4KB)
- **default**: Initial buffer size for new connections (e.g., 65536 = 64KB)
- **max**: Maximum the buffer can grow to dynamically (e.g., 16777216 = 16MB)

**Why it matters:**
- Larger buffers = higher throughput on fast networks
- But also = more memory per connection
- Important for high bandwidth-delay product (BDP) networks

**Bandwidth-Delay Product (BDP):**
```
BDP = Bandwidth √ó RTT
Example: 1 Gbps √ó 100ms = 12.5 MB buffer needed for full utilization
```

**Our configurations:**
- Gaming: `"4096 87380 16777216"` (16MB max) - moderate for low latency
- Streaming: `"8192 262144 67108864"` (64MB max) - large for throughput
- Server: `"8192 262144 67108864"` (64MB max) - balanced

#### tcp_wmem (Write/Send Memory)
Same concept as tcp_rmem but for outgoing data.

#### core_rmem_max & core_wmem_max
**What they are:** System-wide maximum buffer sizes
**Relationship:** tcp_rmem/wmem max values cannot exceed these

### 3.3 TCP Window Scaling (RFC 1323)

**What it is:** Allows TCP windows larger than 64KB

**Why it matters:**
- Original TCP header limits window size to 65,535 bytes
- Modern networks need much larger windows (megabytes)
- Window scaling multiplies the window size by a factor

**Configuration:**
```bash
sysctl -w net.ipv4.tcp_window_scaling=1
```

**When to use:** Always on modern networks (default should be enabled)

### 3.4 TCP Timestamps (RFC 1323)

**What it is:** Adds timestamp to each TCP packet

**Benefits:**
- More accurate RTT (Round Trip Time) measurement
- Protection Against Wrapped Sequences (PAWS)
- Required for high-speed networks

**Overhead:** 12 bytes per packet

**Configuration:**
```bash
sysctl -w net.ipv4.tcp_timestamps=1
```

### 3.5 TCP Fast Open (TFO)

**What it is:** Allows data to be sent during TCP handshake

**Normal TCP handshake:**
1. Client ‚Üí Server: SYN
2. Server ‚Üí Client: SYN-ACK
3. Client ‚Üí Server: ACK + Data (first data here)

**With TFO:**
1. Client ‚Üí Server: SYN + Data (data sent immediately!)
2. Server ‚Üí Client: SYN-ACK + Response
3. Client ‚Üí Server: ACK

**Latency savings:** 1 RTT (round-trip time)

**Values:**
- `0`: Disabled
- `1`: Client mode only
- `2`: Server mode only
- `3`: Both client and server (recommended)

**Security concern:** Vulnerable to replay attacks

**Our usage:** Enabled (`3`) in gaming, video calls, and streaming profiles

### 3.6 TCP Low Latency Mode

**What it is:** Linux kernel optimization that prioritizes latency over throughput

**How it works:**
- Sends data immediately even if it could wait to batch more
- Reduces buffering delays
- May send smaller packets (less efficient but faster)

**Configuration:**
```bash
sysctl -w net.ipv4.tcp_low_latency=1
```

**Our usage:** Gaming profile only

### 3.7 TCP SYN Cookies

**What it is:** Protection against SYN flood attacks

**How it works:**
- Normal: Server allocates memory for each SYN received
- With SYN cookies: Server encodes connection state in the SYN-ACK sequence number
- No memory allocated until ACK received

**Tradeoff:** Slightly slower connection establishment, but prevents DoS

**Configuration:**
```bash
sysctl -w net.ipv4.tcp_syncookies=1
```

**Our usage:** Server profile

### 3.8 TCP FIN Timeout

**What it is:** How long to keep socket in FIN-WAIT-2 state

**Why it matters:**
- Lower value = faster socket reuse
- Too low = may break slow clients
- Default: 60 seconds

**Our configurations:**
- Gaming: 20-30 seconds (faster cleanup)
- Server: 15-30 seconds (handle high connection churn)

### 3.9 TCP Max SYN Backlog

**What it is:** Maximum number of queued connection requests

**Why it matters:**
- Server receiving many connections needs large backlog
- Too small = connection refused errors during traffic spikes

**Our configurations:**
- Gaming/Streaming: 4096-8192 (moderate)
- Server: 8192-16384 (high concurrency)

### 3.10 IP Local Port Range

**What it is:** Range of ports available for outgoing connections

**Default:** `32768-60999` (28,232 ports)

**Why expand:**
- High connection rate services need more ports
- Each TIME_WAIT socket consumes a port

**Our configuration:**
```bash
sysctl -w net.ipv4.ip_local_port_range="1024 65535"
```
This gives 64,511 available ports.

### 3.11 Netdev Budget

**What it is:** Number of packets processed per NAPI poll cycle

**NAPI (New API):** Linux's interrupt mitigation system for network cards

**Why it matters:**
- Higher budget = more packets per cycle
- Better throughput but may increase latency slightly

**Default:** 300
**Our configurations:**
- Streaming: 600 (prioritize throughput)
- Bulk Transfer: 1000-2000 (maximum throughput)

### 3.12 Default Qdisc (Queue Discipline)

**What it is:** System-wide default packet scheduling algorithm

**Options:**
- **pfifo_fast**: Simple FIFO (First In First Out) - Linux default
- **fq (Fair Queue)**: Each flow gets fair share, works best with BBR
- **fq_codel**: Fair queue with CoDel (Controlled Delay) AQM
- **mq**: Multi-queue for multi-core systems

**Configuration:**
```bash
sysctl -w net.core.default_qdisc=fq
```

**Our usage:** `fq` with BBR for optimal performance

### 3.13 IP Forwarding

**What it is:** Allows system to forward packets between interfaces (act as router)

**When needed:**
- NAT/Masquerading
- Router/gateway setups
- VPN servers

**Configuration:**
```bash
sysctl -w net.ipv4.ip_forward=1
```

---

## 4. Traffic Control (TC) Concepts

### 4.1 What is Traffic Control?

**Traffic Control (tc)** is Linux's system for controlling how packets are queued, shaped, and prioritized.

**Key concepts:**
- **Qdisc (Queue Discipline)**: Algorithm for scheduling packets
- **Class**: Subdivision of traffic within a qdisc
- **Filter**: Rules for classifying packets into classes

### 4.2 Qdisc Types

#### 4.2.1 HTB (Hierarchical Token Bucket)

**What it is:** Advanced shaping with hierarchical bandwidth allocation

**Key concepts:**

**Rate vs Ceil:**
- **Rate**: Guaranteed bandwidth (minimum)
- **Ceil**: Maximum bandwidth (ceiling)
- Class can borrow unused bandwidth from parent up to ceil

**Priority:**
- Lower number = higher priority (0-7)
- Determines which class gets bandwidth first

**Classid:**
- Format: `major:minor` (e.g., `1:10`)
- `1:1` = root class
- `1:10`, `1:20`, `1:30` = child classes

**Example:**
```bash
# Create root qdisc
tc qdisc add dev eth0 root handle 1: htb default 30

# Root class (100 Mbps total)
tc class add dev eth0 parent 1: classid 1:1 htb rate 100mbit

# Gaming class (50 Mbps guaranteed, 80 Mbps max, priority 1)
tc class add dev eth0 parent 1:1 classid 1:10 htb rate 50mbit ceil 80mbit prio 1

# Default class (30 Mbps guaranteed, 100 Mbps max, priority 3)
tc class add dev eth0 parent 1:1 classid 1:30 htb rate 30mbit ceil 100mbit prio 3
```

**Our usage:** Gaming and video calls for traffic prioritization

#### 4.2.2 CAKE (Common Applications Kept Enhanced)

**What it is:** All-in-one modern qdisc with built-in shaping, AQM, and fairness

**Features:**
- Bandwidth shaping
- Flow isolation
- Diffserv prioritization
- RTT compensation

**Parameters:**
- **bandwidth**: Total bandwidth limit (e.g., `100mbit`)
- **rtt**: Round-trip time estimate (e.g., `100ms`)
- **diffserv4/diffserv8**: DiffServ mode for QoS marking

**Example:**
```bash
tc qdisc add dev eth0 root cake bandwidth 100mbit rtt 50ms diffserv4
```

**Best for:** ISP edge routers, home gateways

#### 4.2.3 FQ (Fair Queue)

**What it is:** Fair scheduling between flows, works optimally with BBR

**How it works:**
- Each flow (connection) gets its own queue
- Round-robin between flows
- Prevents one connection from monopolizing bandwidth

**Parameters:**
- **pacing**: Enable pacing for BBR
- **maxrate**: Maximum rate per flow

**Our usage:** Default qdisc for all BBR-based profiles

#### 4.2.4 FQ-CoDel (Fair Queue + Controlled Delay)

**What it is:** Fair queueing with active queue management (AQM)

**How it works:**
- Fair queueing between flows (like FQ)
- CoDel algorithm drops packets when queue delay exceeds target

**Parameters:**
- **limit**: Maximum queue size (packets)
- **target**: Target queue delay (e.g., `5ms`)
- **interval**: CoDel measurement interval

**Best for:** Bufferbloat mitigation, gaming networks

#### 4.2.5 pfifo_fast

**What it is:** Simple 3-band FIFO (Linux default)

**How it works:**
- 3 priority bands (0 = highest)
- TOS field determines band
- Simple and fast but no fairness

**When to use:** Low-end systems, minimal CPU overhead

### 4.3 Netem (Network Emulation)

**What it is:** Tool for testing network behavior by adding delay, loss, jitter, etc.

**‚ö†Ô∏è IMPORTANT:** Only for testing/development, NEVER production!

**Parameters:**

#### Delay
```bash
tc qdisc add dev eth0 root netem delay 100ms
```
Adds 100ms latency to all packets.

#### Jitter
```bash
tc qdisc add dev eth0 root netem delay 100ms 20ms
```
Delay varies between 80-120ms.

#### Loss
```bash
tc qdisc add dev eth0 root netem loss 1%
```
Randomly drops 1% of packets.

#### Duplicate
```bash
tc qdisc add dev eth0 root netem duplicate 1%
```
Duplicates 1% of packets.

#### Corrupt
```bash
tc qdisc add dev eth0 root netem corrupt 0.1%
```
Corrupts 0.1% of packets.

**Our usage:** Testing and validation only

### 4.4 Traffic Shaping

**What it is:** Controlling outbound bandwidth

**Why shape:**
- Prevent link saturation
- Guarantee bandwidth for important traffic
- Create predictable network behavior

**Our configuration:**
- Uses HTB with rate/ceil parameters
- Configured per-class for different traffic types

---

## 5. Firewall & Security (iptables/nftables)

### 5.1 iptables vs nftables

**iptables:**
- Legacy Linux firewall (still widely used)
- Separate tools: iptables, ip6tables, arptables, ebtables
- Less efficient for large rulesets

**nftables:**
- Modern replacement for iptables
- Unified tool for all protocols
- Better performance
- More flexible syntax

**Our approach:** Generate nftables scripts for modern systems

### 5.2 Tables and Chains

#### Tables
- **filter**: Packet filtering (accept/drop)
- **nat**: Network Address Translation
- **mangle**: Packet modification (DSCP marking, TTL)
- **raw**: Connection tracking bypass

#### Chains
- **input**: Packets destined for this system
- **output**: Packets originating from this system
- **forward**: Packets passing through (routing)
- **prerouting**: Before routing decision
- **postrouting**: After routing decision

### 5.3 DSCP (Differentiated Services Code Point) Marking

**What it is:** 6-bit field in IP header for QoS classification

**How it works:**
1. Mark packets with DSCP value (0-63)
2. Network equipment reads DSCP
3. Equipment prioritizes packets accordingly

**Common DSCP values:**
- **EF (Expedited Forwarding) = 46**: Highest priority (VoIP, gaming)
- **CS6 (Class Selector 6) = 48**: Network control
- **CS5 (Class Selector 5) = 40**: Video
- **AF41-43 (Assured Forwarding) = 34-38**: Critical data

**nftables example:**
```nftables
table ip mangle {
  chain postrouting {
    type filter hook postrouting priority -150; policy accept;
    
    # Mark gaming UDP traffic with EF
    udp sport 27000-27050 ip dscp set ef counter
    
    # Mark VoIP with CS6
    udp dport 5060-5090 ip dscp set cs6 counter
  }
}
```

**‚ö†Ô∏è Important:** DSCP marking only helps if:
1. Your router/ISP respects DSCP
2. Network equipment is configured to honor DSCP values

**Our usage:** Gaming and video calls profiles for traffic prioritization

### 5.4 Connection Limiting

**What it is:** Limit concurrent connections per IP address

**Why use it:**
- Prevent DoS attacks
- Protect against resource exhaustion
- Ensure fair resource allocation

**nftables example:**
```nftables
# Limit HTTP to 20 connections per IP
tcp dport 80 ct state new add @connlimit_{ip saddr} { ip saddr ct count over 20 } drop
```

**Parameters:**
- **limit**: Maximum connections (e.g., 20)
- **mask**: CIDR mask (32 = per IP, 24 = per /24 subnet)
- **protocol**: tcp/udp
- **port**: Which port to protect

**When to use:**
- Public web servers
- SSH protection (3-5 connections)
- API endpoints

**When to avoid:**
- Behind load balancer/CDN (all traffic appears from CDN IP)
- NAT scenarios (multiple users share one public IP)

### 5.5 Rate Limiting

**What it is:** Limit packet acceptance rate

**Difference from connection limiting:**
- Connection limiting: Max X connections at once
- Rate limiting: Max X packets/second

**nftables example:**
```nftables
# Accept max 150 packets/second
limit rate over 150/second burst 10 packets drop
```

**Parameters:**
- **rate**: e.g., `150/second`, `1000/minute`
- **burst**: Allow bursts up to this many packets

**Use cases:**
- DDoS protection
- Smooth traffic flow
- Prevent packet floods

### 5.6 NAT (Network Address Translation)

**What it is:** Rewriting IP addresses in packets

#### Types:

**SNAT (Source NAT):**
- Changes source IP address
- Used for outbound traffic
- Enables internet sharing

**DNAT (Destination NAT):**
- Changes destination IP address
- Port forwarding
- Load balancing

**Masquerading:**
- Dynamic SNAT using interface IP
- Best for dynamic IP addresses (DHCP, PPPoE)

**nftables example:**
```nftables
table ip nat {
  chain postrouting {
    type nat hook postrouting priority 100; policy accept;
    
    # Masquerade outbound traffic on eth0
    oifname eth0 masquerade
  }
}
```

**Prerequisite:** IP forwarding must be enabled!
```bash
sysctl -w net.ipv4.ip_forward=1
```

### 5.7 Connection Tracking

**What it is:** Linux kernel tracks connection state

**States:**
- **NEW**: First packet of new connection
- **ESTABLISHED**: Part of existing connection
- **RELATED**: Related to existing connection (FTP data channel)
- **INVALID**: Doesn't match any connection

**Parameters:**

**nf_conntrack_max:**
- Maximum tracked connections
- Default: Usually 65536
- Increase for high-traffic servers

**tcp_timeout_established:**
- How long to track idle TCP connections
- Default: 432000 seconds (5 days)
- Lower for high connection churn

**Configuration:**
```bash
sysctl -w net.netfilter.nf_conntrack_max=1000000
sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=3600
```

**Our usage:** Server profile for high concurrency

---

## 6. Network Performance Concepts

### 6.1 Latency

**What it is:** Time for packet to travel from source to destination

**Measured in:** Milliseconds (ms)

**Types:**
- **RTT (Round-Trip Time)**: Time for packet to go and return
- **One-way latency**: Source to destination only
- **p95/p99 latency**: 95th/99th percentile (handles outliers)

**Good values:**
- Gaming: <20ms (p95)
- Video calls: <150ms (ITU-T G.114 standard)
- Web browsing: <200ms

**Factors affecting latency:**
- Physical distance
- Number of hops (routers)
- Bufferbloat
- Congestion

### 6.2 Jitter

**What it is:** Variation in latency

**Example:**
- Without jitter: 50ms, 50ms, 50ms, 50ms (consistent)
- With jitter: 50ms, 80ms, 30ms, 90ms (varies)

**Why it matters:**
- Real-time apps need consistency
- Voice/video use jitter buffers to smooth

**Good values:**
- Gaming: <5ms
- Video calls: <20ms
- Streaming: <50ms (less critical due to buffering)

### 6.3 Packet Loss

**What it is:** Percentage of packets that don't arrive

**Causes:**
- Network congestion
- Hardware errors
- Wireless interference

**Good values:**
- Gaming: <0.01% (near zero)
- Video calls: <0.1%
- Bulk transfer: <0.5% (TCP retransmits)

**Impact:**
- TCP: Retransmission (throughput reduction)
- UDP: Lost forever (degraded quality)

### 6.4 Throughput vs Bandwidth

**Bandwidth:** Maximum theoretical capacity (e.g., 1 Gbps link)

**Throughput:** Actual data transfer rate achieved

**Why different:**
- Protocol overhead (TCP/IP headers)
- Packet loss causing retransmissions
- Congestion
- Window size limitations

**Example:** 1 Gbps link might achieve 940 Mbps throughput (94%)

### 6.5 Bufferbloat

**What it is:** Excessive buffering causing high latency

**How it happens:**
1. Link becomes saturated
2. Packets queue in large buffers
3. Queue delays become very large (seconds!)
4. Result: High latency while link is busy

**Solutions:**
- Active Queue Management (AQM): CoDel, PIE
- Fair queueing: FQ, FQ-CoDel
- Proper shaping: Keep queues small

**Testing:** 
- dslreports.com speed test (shows bufferbloat grade)
- Ping while downloading (latency spike = bufferbloat)

### 6.6 Bandwidth-Delay Product (BDP)

**Formula:** `BDP = Bandwidth √ó RTT`

**What it means:** Amount of data "in flight" on the network

**Example:**
- 100 Mbps √ó 100ms RTT = 12.5 MB
- Need 12.5 MB buffers for full utilization

**Why it matters:** TCP window size must be ‚â• BDP for full throughput

### 6.7 Flow Isolation

**What it is:** Preventing one connection from affecting others

**Techniques:**
- Fair queueing (FQ, FQ-CoDel)
- HTB with per-flow classes
- CAKE's flow isolation

**Benefits:**
- Download doesn't kill gaming latency
- One connection can't monopolize bandwidth

---

## 7. Optimization Profiles

### 7.1 Gaming Profile

**Goal:** Ultra-low latency (<20ms p95)

**Key optimizations:**
- **BBR congestion control**: Minimizes queue buildup
- **tcp_low_latency=1**: Prioritize latency over efficiency
- **tcp_fastopen=3**: Reduce connection setup time
- **FQ qdisc**: Fair queueing prevents bufferbloat
- **Small buffers**: 16MB max (prevents excessive buffering)
- **DSCP marking**: Mark game traffic with EF (46)

**Research basis:**
- Valve Source Engine documentation (15ms tick rate)
- BBR research (Google, 2016)
- RFC 1323 (TCP extensions)

**Target applications:** FPS games, MOBAs, real-time multiplayer

### 7.2 Streaming Profile

**Goal:** Maximum throughput

**Key optimizations:**
- **BBR congestion control**: Best for high throughput
- **Large buffers**: 64MB+ (maximize BDP)
- **tcp_window_scaling=1**: Support large windows
- **Netdev budget=600**: Process more packets per cycle
- **FQ qdisc**: Works well with BBR

**Research basis:**
- BBR achieves 2-25√ó better throughput on bufferbloated paths
- RFC 1323 (high-performance TCP)
- Linux kernel scaling documentation

**Target applications:** Video streaming, content delivery, large downloads

### 7.3 Video Calls Profile

**Goal:** Balanced latency (<150ms) and throughput (2+ Mbps)

**Key optimizations:**
- **BBR congestion control**: Balanced performance
- **Moderate buffers**: 32MB (balance latency/throughput)
- **tcp_fastopen=3**: Faster connection setup
- **FQ qdisc**: Low latency queueing
- **DSCP marking**: Prioritize video traffic

**Research basis:**
- ITU-T G.114: 150ms one-way transmission time limit
- RFC 7478: WebRTC requirements
- WebRTC standards

**Target applications:** Zoom, Teams, WebRTC applications

### 7.4 Bulk Transfer Profile

**Goal:** Absolute maximum throughput

**Key optimizations:**
- **BBR congestion control**: Aggressive bandwidth probing
- **Huge buffers**: 128MB-512MB (handle massive BDP)
- **tcp_window_scaling=1**: Essential for large transfers
- **Netdev budget=1000-2000**: Maximum packet processing
- **Wide port range**: 1024-65535 (avoid port exhaustion)

**Research basis:**
- RFC 1323 (high-performance TCP)
- BBR research
- Linux kernel high-performance networking docs

**Target applications:** Backups, replication, large file transfers

### 7.5 Server Profile

**Goal:** High concurrency (10K+ connections) + security

**Key optimizations:**
- **BBR congestion control**: Fair service across connections
- **tcp_syncookies=1**: SYN flood protection
- **max_syn_backlog=16384**: Handle connection bursts
- **Connection tracking**: 1M+ tracked connections
- **Connection/rate limiting**: DoS protection
- **Short FIN timeout**: 15-30s (faster cleanup)

**Research basis:**
- C10K problem solutions
- Linux kernel server tuning guides
- DDoS mitigation best practices

**Target applications:** Web servers, APIs, database servers

---

## 8. System Architecture

### 8.1 Component Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ LLM (Claude)‚îÇ ‚Üê User talks to AI
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ MCP Protocol (JSON-RPC)
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ MCP Server (FastMCP)                ‚îÇ
‚îÇ ‚îú‚îÄ Registry (registers tools)       ‚îÇ
‚îÇ ‚îú‚îÄ Resources (policy cards)         ‚îÇ
‚îÇ ‚îî‚îÄ Tools (functions LLM can call)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Processing Pipeline                 ‚îÇ
‚îÇ 1. Validate ‚Üí 2. Plan ‚Üí 3. Render  ‚îÇ
‚îÇ        ‚Üì            ‚Üì          ‚Üì    ‚îÇ
‚îÇ   Check rules  Translate  Generate  ‚îÇ
‚îÇ                           Commands   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Apply Layer (with rollback)         ‚îÇ
‚îÇ ‚îú‚îÄ Checkpoint current state         ‚îÇ
‚îÇ ‚îú‚îÄ Execute commands                 ‚îÇ
‚îÇ ‚îî‚îÄ Rollback on failure              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Linux Kernel                        ‚îÇ
‚îÇ ‚îú‚îÄ sysctl (TCP/IP parameters)      ‚îÇ
‚îÇ ‚îú‚îÄ tc (traffic control)             ‚îÇ
‚îÇ ‚îú‚îÄ nftables (firewall/QoS)          ‚îÇ
‚îÇ ‚îî‚îÄ ethtool (NIC offloads)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 8.2 Data Flow

**1. User Request ‚Üí ParameterPlan**
```json
{
  "iface": "eth0",
  "profile": "gaming",
  "changes": {
    "sysctl": {
      "net.ipv4.tcp_congestion_control": "bbr"
    },
    "qdisc": {
      "type": "fq"
    }
  }
}
```

**2. Validation ‚Üí ValidationResult**
- Check Pydantic schema
- Verify against policy limits
- Ensure interface exists

**3. Rendering ‚Üí RenderedPlan**
```json
{
  "sysctl_cmds": [
    "sysctl -w net.ipv4.tcp_congestion_control=bbr"
  ],
  "tc_script": "tc qdisc del dev eth0 root\ntc qdisc add dev eth0 root fq"
}
```

**4. Apply ‚Üí ChangeReport**
```json
{
  "applied": true,
  "checkpoint_id": "cp_1699632000",
  "errors": [],
  "notes": ["Created checkpoint", "Applied sysctl", "Applied tc"]
}
```

### 8.3 Safety Mechanisms

**1. Command Allowlisting**
- Only approved binaries can execute
- Located in: `server/config/allowlist.yaml`

**2. Checkpoint/Rollback**
- Snapshot state before changes
- Automatic rollback on any error
- Manual rollback available

**3. Schema Validation**
- Pydantic models enforce types
- Invalid data rejected before execution

**4. Policy Enforcement**
- `policy/limits.yaml`: Maximum values
- `policy/rules.yaml`: Conditional adjustments
- `policy/validation_limits.yaml`: Validation boundaries

**5. Atomic Operations**
- All commands succeed or all rollback
- No partial state changes

---

## 9. Common Technical Questions & Answers

### Q1: Why BBR instead of CUBIC?

**Answer:**
BBR (Bottleneck Bandwidth and Round-trip propagation time) is a modern congestion control algorithm developed by Google in 2016. Unlike CUBIC which relies on packet loss to detect congestion, BBR actively measures available bandwidth and round-trip time.

**Key advantages:**
- 2-25√ó higher throughput on bufferbloated networks
- Lower latency under congestion
- Better performance on variable networks (WiFi, mobile)
- Doesn't need packet loss to back off

**Research:** Neal Cardwell et al., "BBR: Congestion-Based Congestion Control," ACM Queue, 2016

**When CUBIC might be better:**
- Very old networks that don't handle BBR well
- When BBR isn't available in kernel

### Q2: What is bufferbloat and how do you fix it?

**Answer:**
Bufferbloat occurs when excessive buffering in network equipment causes high latency. When a link becomes saturated, packets queue in large buffers, causing delays that can reach several seconds.

**How we fix it:**
1. **Fair queueing**: FQ, FQ-CoDel isolate flows
2. **Active Queue Management**: CoDel drops packets when delay is excessive
3. **Proper shaping**: Keep queues small by shaping before bottleneck
4. **Smart qdisc**: CAKE combines all the above

**Symptoms:**
- High latency during downloads/uploads
- Gaming lag when someone streams
- Video call stuttering with background traffic

**Testing:** dslreports.com speed test shows bufferbloat grade (A-F)

### Q3: What's the difference between HTB rate and ceil?

**Answer:**
In HTB (Hierarchical Token Bucket):

**Rate (guaranteed bandwidth):**
- Minimum bandwidth guaranteed to class
- Always available even under congestion
- Sum of all rates should ‚â§ total bandwidth

**Ceil (ceiling/maximum bandwidth):**
- Maximum bandwidth class can use
- Can borrow unused bandwidth from parent up to ceil
- Allows bursting when bandwidth available

**Example:**
```bash
# Gaming class: 50 Mbps guaranteed, 80 Mbps max
tc class add dev eth0 parent 1:1 classid 1:10 htb rate 50mbit ceil 80mbit

# Default class: 30 Mbps guaranteed, 100 Mbps max
tc class add dev eth0 parent 1:1 classid 1:30 htb rate 30mbit ceil 100mbit
```

If gaming needs only 40 Mbps, default class can use up to 100 Mbps total.
If both need bandwidth, gaming gets 50 Mbps, default gets 30 Mbps.

### Q4: Why do you need IP forwarding for NAT?

**Answer:**
IP forwarding allows the Linux kernel to forward packets between network interfaces, essentially acting as a router.

**Without IP forwarding:**
- Packets destined for other networks are dropped
- System only handles traffic for itself

**With IP forwarding enabled:**
- Packets can flow from one interface to another
- NAT can rewrite addresses and forward to internet

**NAT workflow:**
1. Client sends packet (src=192.168.1.5, dst=8.8.8.8)
2. IP forwarding moves it from LAN interface to WAN interface
3. NAT rewrites source address (src=public_ip, dst=8.8.8.8)
4. Packet goes to internet
5. Response comes back, NAT rewrites destination back to 192.168.1.5
6. IP forwarding sends it back to LAN interface

**Enable:**
```bash
sysctl -w net.ipv4.ip_forward=1
```

### Q5: What is DSCP marking and when is it useful?

**Answer:**
DSCP (Differentiated Services Code Point) is a 6-bit field in the IP header used for Quality of Service (QoS) classification.

**How it works:**
1. Your system marks packets with DSCP value (0-63)
2. Routers/switches read DSCP value
3. Equipment prioritizes packets based on DSCP

**Common values:**
- **EF (46)**: Expedited Forwarding - highest priority (VoIP, gaming)
- **AF41-43 (34-38)**: Assured Forwarding - important data
- **CS5 (40)**: Video streaming
- **CS0 (0)**: Best effort (default)

**‚ö†Ô∏è Important limitations:**
- Only works if your router/ISP respects DSCP
- Many consumer routers ignore DSCP
- ISPs may overwrite DSCP values at edge
- Most effective on enterprise/managed networks

**When useful:**
- Corporate networks with QoS-aware equipment
- You control the entire network path
- Gaming on managed networks

**When NOT useful:**
- Consumer internet connections (ISP ignores it)
- Unmanaged home routers
- Public internet (DSCP stripped/rewritten)

### Q6: How do TCP buffers affect performance?

**Answer:**
TCP buffers store data waiting to be sent or received. Buffer size directly impacts throughput, especially on high-bandwidth or high-latency networks.

**The Bandwidth-Delay Product (BDP):**
```
BDP = Bandwidth √ó RTT
```

**Example:**
- 1 Gbps connection
- 100ms RTT
- BDP = 1,000 Mbps √ó 0.1s = 100 Megabits = 12.5 MB

You need at least 12.5 MB buffers to fully utilize the link!

**Buffer sizes:**
- **Too small**: Can't utilize full bandwidth (throughput limited)
- **Too large**: More memory per connection, fewer possible connections
- **Just right**: Matches your BDP requirements

**Our approach:**
- Gaming: Smaller buffers (16MB) - prevent latency
- Streaming: Large buffers (64MB) - maximize throughput
- Server: Balanced (64MB) - handle many connections

### Q7: What's the difference between connection limiting and rate limiting?

**Answer:**

**Connection Limiting:**
- Limits **number of concurrent connections** per IP
- Example: Max 20 HTTP connections per IP
- Protects against connection exhaustion
- Measured in: connections

**Rate Limiting:**
- Limits **packet/request rate** 
- Example: Max 150 packets/second
- Protects against packet floods
- Measured in: packets/second or requests/second

**Use cases:**

**Connection Limiting:**
```bash
# Max 20 concurrent HTTP connections per IP
iptables -A INPUT -p tcp --dport 80 -m connlimit --connlimit-above 20 -j REJECT
```
Good for: Preventing DoS, ensuring fair resource allocation

**Rate Limiting:**
```bash
# Max 150 packets/second
iptables -A INPUT -m limit --limit 150/second -j ACCEPT
```
Good for: DDoS protection, smoothing traffic bursts

**Often used together:** Connection limiting protects resources, rate limiting protects bandwidth.

### Q8: Why FQ qdisc with BBR?

**Answer:**
FQ (Fair Queue) and BBR are designed to work together by Google.

**BBR needs pacing:**
- BBR calculates optimal sending rate
- Without pacing, bursts can cause queue buildup
- FQ provides per-flow pacing

**FQ provides fairness:**
- Each flow gets its own queue
- Round-robin between flows
- One connection can't monopolize bandwidth

**Together:**
- BBR determines the rate
- FQ paces the traffic
- Result: Optimal throughput with minimal queue buildup

**Alternative:** fq_codel also works well, adds CoDel AQM on top

**Don't use:** pfifo_fast with BBR (no pacing, defeats BBR's purpose)

### Q9: What are the security risks of TCP Fast Open?

**Answer:**
TCP Fast Open (TFO) allows data in the initial SYN packet, which creates a **replay attack** vulnerability.

**The attack:**
1. Attacker captures SYN packet with TFO cookie
2. Attacker replays the packet multiple times
3. Server processes the same request multiple times

**Example scenario:**
- Bank transfer request in TFO SYN packet
- Attacker replays it 100 times
- 100 transfers executed!

**Mitigations:**
1. **Application-level protection**: Use idempotency keys, nonces
2. **TLS**: Encrypted connections prevent meaningful replay
3. **Server validation**: Check for duplicate requests
4. **Limited scope**: Only enable for trusted applications

**Our approach:**
- Enable TFO for performance (gaming, video calls)
- Rely on application-layer security (TLS, authentication)
- Document the risk in configuration

**When to disable:**
- Untrusted networks
- Sensitive operations without idempotency
- Paranoid security requirements

### Q10: How does your system ensure safety?

**Answer:**
Multiple layers of safety protect against mistakes:

**1. Validation Layer:**
- Pydantic schemas enforce correct types
- Policy limits prevent extreme values
- Interface existence checks

**2. Allowlisting:**
- Only approved commands can execute
- `sysctl`, `tc`, `nft`, `ip` allowlisted
- No arbitrary command execution

**3. Checkpoint/Rollback:**
- Automatic checkpoint before changes
- Any error triggers immediate rollback
- Manual rollback available

**4. Atomic Operations:**
- All commands succeed or all rollback
- No partial configurations
- System never left in inconsistent state

**5. Command Sanitization:**
- Commands passed as argv arrays (no shell injection)
- Parameters validated before rendering
- No user input directly in commands

**6. Audit Trail:**
- Every change logged
- Rationale captured
- Checkpoint history maintained

**Example failure scenario:**
1. User requests change
2. System creates checkpoint
3. Applies sysctl (success)
4. Applies tc (fails!)
5. Automatic rollback to checkpoint
6. User notified of failure
7. Network still works (unchanged)

### Q11: What's the difference between iptables and nftables?

**Answer:**

**iptables (legacy):**
- Separate tools: `iptables`, `ip6tables`, `arptables`, `ebtables`
- One rule = one kernel call (slow for large rulesets)
- Complex syntax
- Still widely used and supported

**nftables (modern):**
- Single tool: `nft`
- Handles IPv4, IPv6, ARP, bridging
- Batch updates (faster)
- More flexible syntax
- Better performance

**Compatibility:**
- Can't use both at once (conflict)
- `iptables-nft` provides iptables syntax with nftables backend
- Gradual migration path

**Our choice:**
- Generate nftables scripts (modern, efficient)
- Can adapt to iptables if needed
- Better performance for complex QoS/NAT rules

**Example comparison:**

iptables:
```bash
iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE
iptables -t mangle -A POSTROUTING -p tcp --dport 80 -j DSCP --set-dscp 46
```

nftables:
```bash
table ip nat {
  chain postrouting {
    type nat hook postrouting priority 100;
    oifname eth0 masquerade
  }
}

table ip mangle {
  chain postrouting {
    type filter hook postrouting priority -150;
    tcp dport 80 ip dscp set ef
  }
}
```

### Q12: Why do you have multiple buffer parameters (tcp_rmem, core_rmem_max)?

**Answer:**
These parameters work together hierarchically:

**core_rmem_max / core_wmem_max:**
- System-wide maximum
- Hard limit that cannot be exceeded
- Affects all sockets (TCP, UDP, etc.)

**tcp_rmem / tcp_wmem:**
- TCP-specific buffer sizes
- Format: "min default max"
- The `max` value cannot exceed `core_*mem_max`

**Relationship:**
```
tcp_rmem max ‚â§ core_rmem_max
```

**Why separate?**
- Different protocols have different needs
- System-wide limit prevents memory exhaustion
- Per-protocol tuning for specific workloads

**Configuration order:**
1. Set `core_rmem_max` first (system limit)
2. Then set `tcp_rmem` (TCP-specific)

**Example:**
```bash
# System limit: 64MB
sysctl -w net.core.core_rmem_max=67108864

# TCP can use up to 64MB (matches system limit)
sysctl -w net.ipv4.tcp_rmem="8192 262144 67108864"
```

If you try to set `tcp_rmem` max higher than `core_rmem_max`, it will be silently capped.

---

## 10. Quick Reference Tables

### 10.1 Congestion Control Algorithms

| Algorithm | Best For | Pros | Cons |
|-----------|----------|------|------|
| **BBR** | High throughput, streaming | 2-25√ó better on bufferbloat, low latency | Requires FQ qdisc, newer |
| **CUBIC** | General purpose | Stable, proven | Poor on bufferbloated networks |
| **Reno** | Legacy compatibility | Simple, compatible | Poor performance on fast networks |
| **DCTCP** | Data centers with ECN | Excellent in DC | Requires ECN support |
| **Hybla** | Satellite/high-latency | Optimized for long RTT | Not for regular networks |
| **Vegas** | Research | RTT-based (proactive) | Less aggressive than loss-based |

### 10.2 Qdisc Types

| Qdisc | Use Case | Complexity | Features |
|-------|----------|------------|----------|
| **FQ** | BBR, general | Medium | Fair queuing, pacing |
| **FQ-CoDel** | Bufferbloat mitigation | Medium | Fair queue + AQM |
| **HTB** | Traffic shaping, priority | High | Hierarchical classes, guarantees |
| **CAKE** | ISP edge, home gateway | Medium | All-in-one shaping/AQM/fairness |
| **pfifo_fast** | Default, simple | Low | 3-band FIFO, minimal overhead |

### 10.3 DSCP Values

| Name | Value | Priority | Use Case |
|------|-------|----------|----------|
| **EF** | 46 | Highest | VoIP, gaming packets |
| **CS6** | 48 | Very High | Network control, gaming voice |
| **CS5** | 40 | High | Video streaming |
| **AF41** | 34 | Medium-High | Important data |
| **AF42** | 36 | Medium-High | Important data |
| **CS4** | 32 | Medium | Business-critical |
| **CS0** | 0 | Best Effort | Default traffic |

### 10.4 Profile Comparison

| Profile | Latency Target | Throughput | Buffer Size | Key Feature |
|---------|----------------|------------|-------------|-------------|
| **Gaming** | <20ms (p95) | 1+ Mbps | 16MB | tcp_low_latency=1 |
| **Streaming** | <100ms | Maximum | 64MB | Large buffers, high netdev_budget |
| **Video Calls** | <150ms | 2+ Mbps | 32MB | Balanced, DSCP marking |
| **Bulk Transfer** | <200ms | Absolute max | 128-512MB | Huge buffers, wide port range |
| **Server** | Variable | High | 64MB | Security, connection limiting |

### 10.5 Common Port Ranges

| Application | Protocol | Ports | DSCP |
|-------------|----------|-------|------|
| Gaming (Steam/Valve) | UDP | 27000-27050 | EF (46) |
| VoIP (SIP) | UDP | 5060-5090 | CS6 (48) |
| WebRTC | UDP | 3478-3497 | CS5 (40) |
| HTTP/HTTPS | TCP | 80, 443 | CS0 (0) |
| SSH | TCP | 22 | CS6 (48) |
| DNS | UDP | 53 | CS5 (40) |

### 10.6 sysctl Parameters Quick Reference

| Parameter | Values | Default | Gaming | Streaming | Server |
|-----------|--------|---------|--------|-----------|--------|
| tcp_congestion_control | bbr/cubic/reno | cubic | bbr | bbr | bbr |
| tcp_window_scaling | 0/1 | 1 | 1 | 1 | 1 |
| tcp_timestamps | 0/1 | 1 | 1 | 1 | 1 |
| tcp_fastopen | 0-3 | 0 | 3 | 3 | 3 |
| tcp_low_latency | 0/1 | 0 | 1 | 0 | 0 |
| tcp_syncookies | 0/1 | 1 | 0 | 0 | 1 |
| tcp_fin_timeout | seconds | 60 | 20 | 30 | 15 |
| tcp_max_syn_backlog | count | 4096 | 8192 | 8192 | 16384 |
| default_qdisc | fq/fq_codel/pfifo_fast | pfifo_fast | fq | fq | fq |
| netdev_budget | count | 300 | 300 | 600 | 300-600 |

---

## 11. Troubleshooting Guide

### Issue: Low throughput despite optimization

**Check:**
1. Is BBR actually active? `sysctl net.ipv4.tcp_congestion_control`
2. Are buffers large enough? `sysctl net.ipv4.tcp_rmem net.ipv4.tcp_wmem`
3. Is window scaling enabled? `sysctl net.ipv4.tcp_window_scaling`
4. Does buffer max exceed core_*mem_max? `sysctl net.core.core_rmem_max`
5. Is qdisc limiting? `tc -s qdisc show dev eth0`

### Issue: High latency in gaming

**Check:**
1. Is bufferbloat present? Test at dslreports.com
2. Is FQ or FQ-CoDel qdisc active? `tc qdisc show`
3. Is tcp_low_latency enabled? `sysctl net.ipv4.tcp_low_latency`
4. Are buffers too large? Check tcp_rmem/wmem
5. Is other traffic on same connection? Check with `iftop` or `nethogs`

### Issue: Connections being refused

**Check:**
1. Is connection limiting too strict? `nft list ruleset | grep connlimit`
2. Is SYN backlog full? `netstat -s | grep -i listen`
3. Is conntrack table full? `sysctl net.netfilter.nf_conntrack_max`
4. Check current conntrack usage: `cat /proc/sys/net/netfilter/nf_conntrack_count`

### Issue: NAT not working

**Check:**
1. Is IP forwarding enabled? `sysctl net.ipv4.ip_forward` (should be 1)
2. Are NAT rules present? `nft list ruleset | grep masquerade`
3. Is correct interface specified? `ip link show`
4. Check conntrack: `cat /proc/net/nf_conntrack`

### Issue: DSCP marking not working

**Check:**
1. Are marks being applied? `nft list ruleset | grep dscp`
2. Does router respect DSCP? (requires QoS-aware equipment)
3. Is ISP stripping DSCP? (test with tcpdump/wireshark)
4. Are packets matching the rules? Check counters: `nft list ruleset`

---

## 12. Key Research Papers & Standards

### RFC Standards
- **RFC 1323**: TCP Extensions for High Performance (window scaling, timestamps)
- **RFC 7478**: Web Real-Time Communication Use Cases and Requirements
- **ITU-T G.114**: One-way transmission time limits (150ms for voice)

### Research Papers
- **BBR paper**: Cardwell et al., "BBR: Congestion-Based Congestion Control," ACM Queue, 2016
- Explains BBR algorithm, shows 2-25√ó improvement on bufferbloated networks

### Documentation
- Linux kernel networking: `Documentation/networking/`
- Valve Source Engine networking documentation
- Google BBR research and deployment papers

---

## 13. Presentation Tips

### For Technical Questions:

1. **Start with the problem**: "Bufferbloat causes high latency when the link is saturated..."
2. **Explain the solution**: "We use FQ-CoDel which combines fair queueing with controlled delay..."
3. **Show the benefit**: "This reduces latency from 500ms to 20ms during downloads"
4. **Add context**: "This is based on research by Kathie Nichols and Van Jacobson..."

### For Demo:

1. **Show baseline**: Run speed test, ping test before optimization
2. **Apply profile**: `optimize_for_gaming(interface='eth0')`
3. **Show improvement**: Run same tests, compare results
4. **Explain the difference**: "Latency improved because..."

### Key Messages:

1. **Safe**: Automatic rollback, validation, checkpoints
2. **Research-backed**: BBR, RFC 1323, ITU-T standards
3. **Comprehensive**: 29 config cards, 5 profiles
4. **AI-native**: LLMs can optimize networks without manual commands

---

## 14. Glossary

**AQM (Active Queue Management)**: Algorithms that actively manage queue lengths (CoDel, PIE)

**BDP (Bandwidth-Delay Product)**: Amount of data "in flight" = Bandwidth √ó RTT

**Bufferbloat**: Excessive buffering causing high latency

**CoDel (Controlled Delay)**: AQM algorithm that targets queue delay

**Conntrack**: Linux connection tracking system

**DSCP (Differentiated Services Code Point)**: QoS marking in IP header (6 bits)

**ECN (Explicit Congestion Notification)**: In-band congestion signaling

**FQ (Fair Queue)**: Fair scheduling between flows

**HTB (Hierarchical Token Bucket)**: Advanced traffic shaping with classes

**MTU (Maximum Transmission Unit)**: Maximum packet size (usually 1500 bytes)

**NAPI (New API)**: Linux network interrupt mitigation

**NAT (Network Address Translation)**: Rewriting IP addresses in packets

**Netem**: Network emulation tool (adds delay, loss, etc.)

**Qdisc (Queue Discipline)**: Packet scheduling algorithm

**RTT (Round-Trip Time)**: Time for packet to go and return

**SYN flood**: DoS attack sending many SYN packets

**TFO (TCP Fast Open)**: Data in initial SYN packet

**TOS/DSCP**: Type of Service / Differentiated Services fields in IP header

---

## 15. Final Checklist for Presentation

### Understand These Core Concepts:
- ‚úÖ What BBR is and why it's better than CUBIC
- ‚úÖ How TCP buffers affect performance (BDP)
- ‚úÖ What bufferbloat is and how to fix it
- ‚úÖ Difference between HTB rate and ceil
- ‚úÖ How DSCP marking works and its limitations
- ‚úÖ Why IP forwarding is needed for NAT
- ‚úÖ How connection tracking works
- ‚úÖ The difference between connection and rate limiting

### Know Your Profiles:
- ‚úÖ Gaming: Ultra-low latency (<20ms)
- ‚úÖ Streaming: Maximum throughput
- ‚úÖ Video Calls: Balanced (150ms, 2+ Mbps)
- ‚úÖ Bulk Transfer: Absolute max throughput
- ‚úÖ Server: High concurrency + security

### Explain Safety:
- ‚úÖ Checkpoint/rollback mechanism
- ‚úÖ Command allowlisting
- ‚úÖ Schema validation
- ‚úÖ Policy enforcement
- ‚úÖ Atomic operations

### Demonstrate Value:
- ‚úÖ 29 configuration cards
- ‚úÖ Research-backed (BBR, RFC 1323, ITU-T G.114)
- ‚úÖ AI-accessible via MCP
- ‚úÖ Production-ready safety features

---

## 16. Common Analogies for Explaining Concepts

### TCP Buffers = Water Tank
- Small tank: Water (data) flows fast but can't store much
- Large tank: Can store lots but may overflow and take time to drain
- Right size: Matches your pipe (bandwidth) size

### BBR vs CUBIC = GPS vs Asking for Directions
- CUBIC: Drive until you hit traffic (packet loss), then slow down
- BBR: Use GPS to see traffic ahead, adjust speed proactively

### Fair Queueing = Checkout Lines
- Without FQ: One person buys 100 items, everyone waits
- With FQ: Separate lines for each person, everyone gets served fairly

### Bufferbloat = Traffic Jam on Bridge
- Bridge (link) has capacity for 100 cars
- Entrance has space for 1000 cars waiting
- When full, 10-minute wait even though bridge is only 1 minute long
- Solution: Limit entrance queue (AQM), prioritize emergency vehicles (QoS)

### DSCP Marking = Express Lane
- Mark important packets (gaming, VoIP) as "express"
- Router reads marking and puts them in fast lane
- Only works if router has express lanes and respects the marking

---

**Good luck with your presentation!** üöÄ

Remember: If you don't know an answer, it's better to say "I don't know, but I can find out" than to guess incorrectly. This guide covers 95% of likely technical questions.
